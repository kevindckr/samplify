import os
import sys
import numpy as np
from collections import deque
from typing import Dict, Tuple, List, Optional

from samplers._sampler_base import BaseImageSampler, numpy_deterministic


class SamplerLiangControlled(BaseImageSampler):
    """
    (This Implementation) Allows Overlap: True

    Article Bibtex Reference:
        @article{Liang2017,
            abstract = {Spectral-spatial processing has been increasingly explored in remote sensing hyperspectral image 
            classification. While extensive studies have focused on developing methods to improve the classification accuracy,
              experimental setting and design for method evaluation have drawn little attention. In the scope of supervised 
              classification, we find that traditional experimental designs for spectral processing are often improperly used 
              in the spectral-spatial processing context, leading to unfair or biased performance evaluation. This is especially 
              the case when training and testing samples are randomly drawn from the same image - a practice that has been 
              commonly adopted in the experiments. Under such setting, the dependence caused by overlap between the training 
              and testing samples may be artificially enhanced by some spatial information processing methods, such as spatial 
              filtering and morphological operation. Such enhancement of dependence in return amplifies the classification 
              accuracy, leading to an improper evaluation of spectral-spatial classification techniques. Therefore, the widely 
              adopted pixel-based random sampling strategy is not always suitable to evaluate spectral-spatial classification 
              algorithms, because it is difficult to determine whether the improvement of classification accuracy is caused 
              by incorporating spatial information into classifier or by increasing the overlap between training and testing 
              samples. To tackle this problem, we propose a novel controlled random sampling strategy for spectral-spatial 
              methods. It can greatly reduce the overlap between training and testing samples and provides more objective 
              and accurate evaluation.},
            author = {Jie Liang and Jun Zhou and Yuntao Qian and Lian Wen and Xiao Bai and Yongsheng Gao},
            doi = {10.1109/TGRS.2016.2616489},
            issn = {01962892},
            issue = {2},
            journal = {IEEE Transactions on Geoscience and Remote Sensing},
            keywords = {Data dependence,experimental setting,hyperspectral image classification,random sampling,
            spectral-spatial processing,supervised learning},
            month = {2},
            pages = {862-880},
            publisher = {Institute of Electrical and Electronics Engineers Inc.},
            title = {On the Sampling Strategy for Evaluation of Spectral-Spatial Methods in Hyperspectral Image Classification},
            volume = {55},
            year = {2017},
        }
    
    Article Sampler Description Excerpt:
    Section VI " First, it selects the unconnected partitions for the each class and counts the samples in each partition. This 
    step is to find the spatial distribution of each class and make sure that the selected training samples in the next step 
    cover the spectral variance at the most extent. Second, for each partition, the training samples are generated by extending 
    region from the seed pixel" ... "This process is repeated until the amount of selected points reach a predefined number, 
    which is proportional to the number of pixels in the corresponding partition. This guarantees that the total number of training 
    samples meets the predefined proportion of the whole data population. Third, after the above steps are applied to all classes, 
    those samples in the grown regions with their labels are chosen, as the training samples and the rest of pixels work as the 
    testing samples. In case when there are more partitions than the required training samples, partitions are again randomly sampled"

    Observations/Notes:
    - It is unclear the exact methodology used to grow regions. It is only noted that "[regions] expands in all directions and 
    takes account of eight-connected neighborhood pixel". This implementation uses a breadth first search on the queue of pixels
    added to the region; starting from the seed.
    - Like Lange, this article allows overlap between train and test samples and faces this issue by forming regions of
    contiguous CPLs that then "protect" inner CPLs from overlap.
    - This method is very data efficient, all possible labelled pixels can be utilized from training and testing.
    """
    def __init__(self,
                 image: np.ndarray, labels: np.ndarray, classes: Dict[int, str],
                 patch_size: Tuple[int, int], train_ratio: float, 
                 name: str, random_seed: Optional[int] = None, **kwargs) -> None:
        super().__init__(image, labels, classes, patch_size, train_ratio, name, random_seed, **kwargs)

    @numpy_deterministic
    def sample(self) -> None:
        print(f'> Executing: {self.name} ({self.__class__.__name__})')

        # Get the initial set of valid center pixel locations for each class
        valid_cpls_per_class, _ = self._initial_cpls_per_class()

        # Calculate all connected components (partitions) for all classes
        print(f'  Calculating all label partitions...')
        partitions_per_class = {}
        for class_idx, class_cpls in valid_cpls_per_class.items():
            partitions_per_class[class_idx] = self.find_connected_components(class_cpls)

        # For each class, for each partition grow a connected training region
        print(f'  Growing training regions in partitions...')
        train_cpls_per_class = {k: np.empty((0, 2), dtype=int) for k in self.classes.keys()}
        test_cpls_per_class = {k: np.empty((0, 2), dtype=int) for k in self.classes.keys()}
        for class_idx, class_partitions in partitions_per_class.items():
            for partition in class_partitions:
                # Grow the region up to size train_ratio using grow_region
                train_region, test_region, _ = self.grow_region(partition, region_ratio=self.train_ratio)

                # Append the train and test CPLs to their respective class arrays
                if train_region.size > 0:
                    train_cpls_per_class[class_idx] = np.vstack([train_cpls_per_class[class_idx], train_region])
                if test_region.size > 0:
                    test_cpls_per_class[class_idx] = np.vstack([test_cpls_per_class[class_idx], test_region])

        # Set the CPLs
        self.training_cpls = train_cpls_per_class
        self.testing_cpls = test_cpls_per_class
